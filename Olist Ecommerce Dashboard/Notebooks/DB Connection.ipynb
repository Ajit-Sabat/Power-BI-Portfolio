{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8007439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "\n",
    "try:\n",
    "    # Connect to MySQL database\n",
    "    connection = mysql.connector.connect(\n",
    "        host='localhost',           # your host\n",
    "        user='root',       # your username\n",
    "        password='root',   # your password\n",
    "        database='ecom_data',   # your database name\n",
    "        allow_local_infile=True     # enable local infile loading\n",
    "    )\n",
    "\n",
    "    if connection.is_connected():\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Path to your CSV file (change as required)\n",
    "        file_path = r'C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Uploads\\dim_customers.csv'\n",
    "\n",
    "        # SQL query to load data from file into the table\n",
    "        load_sql = f\"\"\"\n",
    "        LOAD DATA LOCAL INFILE '{file_path}'\n",
    "        INTO TABLE dim_customers\n",
    "        FIELDS TERMINATED BY ',' \n",
    "        ENCLOSED BY '\"'\n",
    "        LINES TERMINATED BY '\\\\n'\n",
    "        IGNORE 1 LINES;\n",
    "        \"\"\"\n",
    "\n",
    "        cursor.execute(load_sql)\n",
    "        connection.commit()\n",
    "        print(\"Data loaded successfully from file\")\n",
    "\n",
    "except Error as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "finally:\n",
    "    if 'connection' in locals() and connection.is_connected():\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"MySQL connection closed\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56333ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    connection = mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='root',\n",
    "        database='ecom_data',\n",
    "        allow_local_infile=True\n",
    "    )\n",
    "    if connection.is_connected():\n",
    "        cursor = connection.cursor()\n",
    "        file_path = r'C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Uploads\\dim_customers.csv'\n",
    "        load_sql = f\"\"\"\n",
    "        LOAD DATA LOCAL INFILE '{file_path}'\n",
    "        INTO TABLE dim_customers\n",
    "        FIELDS TERMINATED BY ',' \n",
    "        ENCLOSED BY '\"'\n",
    "        LINES TERMINATED BY '\\\\n'\n",
    "        IGNORE 1 LINES;\n",
    "        \"\"\"\n",
    "        cursor.execute(load_sql)\n",
    "        connection.commit()\n",
    "        print(\"Data loaded successfully from file\")\n",
    "\n",
    "except Error as e:\n",
    "    print(\"MySQL error:\", e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error:\", e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    if 'connection' in locals() and connection.is_connected():\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"MySQL connection closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e42354e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement caas_jupyter_tools (from versions: none)\n",
      "ERROR: No matching distribution found for caas_jupyter_tools\n"
     ]
    }
   ],
   "source": [
    "pip install caas_jupyter_tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9272d99e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'caas_jupyter_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaas_jupyter_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display_dataframe_to_user\n\u001b[0;32m     11\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     12\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'caas_jupyter_tools'"
     ]
    }
   ],
   "source": [
    "# Synthetic Olist-style dataset generator (2018-09 to 2023-12)\n",
    "# This script creates CSVs matching the Olist schema with realistic macro effects (COVID-19, supply chain, inflation, recovery).\n",
    "# Files will be saved under /mnt/data/olist_synthetic/ and zipped for easy download.\n",
    "\n",
    "import os, io, zipfile, math, random, string, json\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from caas_jupyter_tools import display_dataframe_to_user\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "OUT_DIR = r\"D:\\Portfolio Projects\\Ecommerce\\Data\\olist_synthetic\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) CONFIG\n",
    "# -----------------------------\n",
    "\n",
    "start_date = pd.Timestamp(\"2018-09-01\")\n",
    "end_date   = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "# Regions & states of Brazil (abridged but broad coverage)\n",
    "states = [\n",
    "    (\"SP\", \"Southeast\"), (\"RJ\", \"Southeast\"), (\"MG\", \"Southeast\"), (\"ES\", \"Southeast\"),\n",
    "    (\"PR\", \"South\"), (\"SC\", \"South\"), (\"RS\", \"South\"),\n",
    "    (\"BA\", \"Northeast\"), (\"PE\", \"Northeast\"), (\"CE\", \"Northeast\"), (\"RN\", \"Northeast\"), (\"PB\", \"Northeast\"), (\"AL\", \"Northeast\"), (\"MA\", \"Northeast\"),\n",
    "    (\"DF\", \"Central-West\"), (\"GO\", \"Central-West\"), (\"MT\", \"Central-West\"), (\"MS\", \"Central-West\"),\n",
    "    (\"PA\", \"North\"), (\"AM\", \"North\"), (\"RO\", \"North\"), (\"RR\", \"North\"), (\"AP\", \"North\"), (\"AC\", \"North\"), (\"TO\", \"North\"),\n",
    "    (\"PI\", \"Northeast\"), (\"SE\", \"Northeast\")\n",
    "]\n",
    "\n",
    "# Some Brazilian city names (sample; synthetic)\n",
    "cities_by_state = {\n",
    "    \"SP\": [\"São Paulo\",\"Campinas\",\"Santos\",\"São José dos Campos\",\"Ribeirão Preto\",\"Sorocaba\"],\n",
    "    \"RJ\": [\"Rio de Janeiro\",\"Niterói\",\"Duque de Caxias\",\"Nova Iguaçu\",\"Petrópolis\"],\n",
    "    \"MG\": [\"Belo Horizonte\",\"Uberlândia\",\"Contagem\",\"Juiz de Fora\",\"Uberaba\"],\n",
    "    \"ES\": [\"Vitória\",\"Vila Velha\",\"Serra\",\"Cariacica\"],\n",
    "    \"PR\": [\"Curitiba\",\"Londrina\",\"Maringá\",\"Foz do Iguaçu\"],\n",
    "    \"SC\": [\"Florianópolis\",\"Joinville\",\"Blumenau\",\"Itajaí\"],\n",
    "    \"RS\": [\"Porto Alegre\",\"Caxias do Sul\",\"Pelotas\",\"Canoas\"],\n",
    "    \"BA\": [\"Salvador\",\"Feira de Santana\",\"Vitória da Conquista\",\"Ilhéus\"],\n",
    "    \"PE\": [\"Recife\",\"Olinda\",\"Caruaru\",\"Petrolina\"],\n",
    "    \"CE\": [\"Fortaleza\",\"Caucaia\",\"Juazeiro do Norte\",\"Sobral\"],\n",
    "    \"RN\": [\"Natal\",\"Mossoró\",\"Parnamirim\"],\n",
    "    \"PB\": [\"João Pessoa\",\"Campina Grande\",\"Patos\"],\n",
    "    \"AL\": [\"Maceió\",\"Arapiraca\",\"Palmeira dos Índios\"],\n",
    "    \"MA\": [\"São Luís\",\"Imperatriz\",\"Caxias\"],\n",
    "    \"DF\": [\"Brasília\"],\n",
    "    \"GO\": [\"Goiânia\",\"Aparecida de Goiânia\",\"Anápolis\"],\n",
    "    \"MT\": [\"Cuiabá\",\"Várzea Grande\",\"Rondonópolis\"],\n",
    "    \"MS\": [\"Campo Grande\",\"Dourados\",\"Três Lagoas\"],\n",
    "    \"PA\": [\"Belém\",\"Ananindeua\",\"Santarém\"],\n",
    "    \"AM\": [\"Manaus\",\"Parintins\",\"Itacoatiara\"],\n",
    "    \"RO\": [\"Porto Velho\",\"Ji-Paraná\"],\n",
    "    \"RR\": [\"Boa Vista\"],\n",
    "    \"AP\": [\"Macapá\"],\n",
    "    \"AC\": [\"Rio Branco\"],\n",
    "    \"TO\": [\"Palmas\",\"Araguaína\"],\n",
    "    \"PI\": [\"Teresina\",\"Parnaíba\"],\n",
    "    \"SE\": [\"Aracaju\",\"Nossa Senhora do Socorro\"]\n",
    "}\n",
    "\n",
    "# Product categories (subset of original Olist; add some)\n",
    "categories = [\n",
    "    (\"bed_bath_table\", 80, 600),\n",
    "    (\"health_beauty\", 30, 700),\n",
    "    (\"sports_leisure\", 50, 1000),\n",
    "    (\"computers_accessories\", 100, 2500),\n",
    "    (\"furniture_decor\", 120, 2000),\n",
    "    (\"housewares\", 40, 900),\n",
    "    (\"watches_gifts\", 90, 1800),\n",
    "    (\"telephony\", 120, 3000),\n",
    "    (\"garden_tools\", 40, 900),\n",
    "    (\"auto\", 70, 1500),\n",
    "    (\"toys\", 30, 600),\n",
    "    (\"pet_shop\", 15, 400),\n",
    "    (\"baby\", 20, 500),\n",
    "    (\"stationery\", 10, 300),\n",
    "    (\"fashion_bags_accessories\", 20, 700),\n",
    "    (\"cool_stuff\", 15, 800),\n",
    "    (\"luggage_accessories\", 60, 1200),\n",
    "    (\"kitchen_dining_laundry_garden_furniture\", 70, 2200),\n",
    "]\n",
    "\n",
    "# Category name translations (simplified)\n",
    "translations = {c[0]: c[0].replace(\"_\",\" \") for c in categories}\n",
    "\n",
    "# Macro scenario multipliers by month (affect orders, delivery delays, cancellations, reviews, prices)\n",
    "# Base monthly order demand (relative)\n",
    "date_range = pd.date_range(start_date, end_date, freq=\"D\")\n",
    "monthly_index = pd.Series(1.0, index=pd.period_range(start=start_date, end=end_date, freq=\"M\"))\n",
    "\n",
    "def set_period_multiplier(start, end, factor):\n",
    "    p = pd.period_range(start, end, freq=\"M\")\n",
    "    for m in p:\n",
    "        monthly_index.loc[m] *= factor\n",
    "\n",
    "# Baseline: gentle growth 2019–2023\n",
    "for m in monthly_index.index:\n",
    "    year = m.start_time.year\n",
    "    if year == 2019:\n",
    "        monthly_index.loc[m] *= 1.05\n",
    "    elif year == 2020:\n",
    "        monthly_index.loc[m] *= 1.10  # more online shopping overall\n",
    "    elif year == 2021:\n",
    "        monthly_index.loc[m] *= 1.15\n",
    "    elif year == 2022:\n",
    "        monthly_index.loc[m] *= 1.12\n",
    "    elif year == 2023:\n",
    "        monthly_index.loc[m] *= 1.18\n",
    "\n",
    "# COVID shock: 2020-03 to 2020-05 dip (supply/logistics issues), then surge 2020-06..2020-12\n",
    "set_period_multiplier(\"2020-03\", \"2020-05\", 0.80)\n",
    "set_period_multiplier(\"2020-06\", \"2020-12\", 1.15)\n",
    "\n",
    "# 2021 supply-chain constraints: demand high but fulfillment slower (we'll model delays elsewhere)\n",
    "set_period_multiplier(\"2021-01\", \"2021-08\", 1.05)\n",
    "\n",
    "# 2022 inflation cools demand a bit\n",
    "set_period_multiplier(\"2022-01\", \"2022-12\", 0.97)\n",
    "\n",
    "# Seasonality: holiday peaks (Black Friday/Cyber Week + Christmas)\n",
    "for m in monthly_index.index:\n",
    "    if m.start_time.month == 11:\n",
    "        monthly_index.loc[m] *= 1.30\n",
    "    if m.start_time.month == 12:\n",
    "        monthly_index.loc[m] *= 1.25\n",
    "\n",
    "# Carnival (Feb) modest lift for certain categories\n",
    "for m in monthly_index.index:\n",
    "    if m.start_time.month == 2:\n",
    "        monthly_index.loc[m] *= 1.05\n",
    "\n",
    "# Price inflation factors by year\n",
    "inflation_by_year = {2018: 1.00, 2019: 1.04, 2020: 1.06, 2021: 1.12, 2022: 1.20, 2023: 1.25}\n",
    "\n",
    "# Logistics delay baseline (days) per year (COVID/supply chain)\n",
    "delay_year_base = {2018: 10, 2019: 9, 2020: 13, 2021: 14, 2022: 11, 2023: 9}\n",
    "\n",
    "# Cancellation rate (portion of orders) by year\n",
    "cancel_rate_year = {2018: 0.03, 2019: 0.025, 2020: 0.04, 2021: 0.035, 2022: 0.03, 2023: 0.025}\n",
    "\n",
    "# Review score mean shift by year (lower during 2020–2021)\n",
    "review_mean_by_year = {2018: 4.1, 2019: 4.2, 2020: 3.9, 2021: 3.95, 2022: 4.05, 2023: 4.15}\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Helper Generators\n",
    "# -----------------------------\n",
    "\n",
    "def rand_id(prefix, n=32):\n",
    "    return prefix + \"_\" + \"\".join(random.choices(string.ascii_lowercase + string.digits, k=n))\n",
    "\n",
    "def pick_state_city():\n",
    "    st, region = random.choice(states)\n",
    "    city = random.choice(cities_by_state[st])\n",
    "    return st, city\n",
    "\n",
    "def zip_prefix():\n",
    "    return int(random.uniform(10000, 99999))\n",
    "\n",
    "def geo_coord_for_state(st):\n",
    "    # rough bounding boxes (synthetic)\n",
    "    boxes = {\n",
    "        \"SP\": (-24.0, -45.0, -20.0, -47.0),\n",
    "        \"RJ\": (-23.4, -43.7, -21.5, -41.0),\n",
    "        \"MG\": (-20.0, -48.5, -14.0, -40.0),\n",
    "        \"RS\": (-33.8, -57.6, -27.0, -49.7),\n",
    "        \"SC\": (-28.5, -53.7, -26.0, -48.4),\n",
    "        \"PR\": (-26.3, -54.6, -22.5, -48.0),\n",
    "        \"BA\": (-18.0, -46.0, -8.0, -38.0),\n",
    "        \"PE\": (-9.6, -38.0, -7.5, -34.8),\n",
    "        \"CE\": (-7.4, -41.4, -2.8, -37.0),\n",
    "        \"PA\": (-5.0, -55.0, 1.0, -47.0),\n",
    "        \"AM\": (-7.0, -63.0, 0.0, -59.0),\n",
    "        \"ES\": (-21.3, -41.2, -18.0, -39.0),\n",
    "        \"GO\": (-18.5, -52.5, -15.0, -48.5),\n",
    "        \"DF\": (-16.1, -48.2, -15.6, -47.7),\n",
    "        \"MT\": (-16.7, -58.7, -12.5, -54.7),\n",
    "        \"MS\": (-22.6, -56.9, -19.6, -53.2),\n",
    "        \"RO\": (-13.9, -65.0, -8.5, -60.5),\n",
    "        \"RR\": (1.5, -61.5, 4.5, -59.5),\n",
    "        \"AP\": (0.0, -52.1, 2.7, -50.6),\n",
    "        \"AC\": (-11.0, -70.0, -7.0, -67.0),\n",
    "        \"TO\": (-12.5, -50.5, -8.0, -45.5),\n",
    "        \"RN\": (-6.4, -36.9, -5.1, -34.8),\n",
    "        \"PB\": (-7.4, -37.7, -6.0, -34.8),\n",
    "        \"AL\": (-10.8, -38.3, -9.2, -35.0),\n",
    "        \"MA\": (-5.7, -47.5, -1.3, -42.8),\n",
    "        \"PI\": (-9.4, -46.5, -2.9, -41.0),\n",
    "        \"SE\": (-11.6, -38.2, -10.0, -36.4),\n",
    "    }\n",
    "    lat1, lng1, lat2, lng2 = boxes.get(st, (-23.0, -47.0, -22.0, -46.0))\n",
    "    lat = random.uniform(min(lat1, lat2), max(lat1, lat2))\n",
    "    lng = random.uniform(min(lng1, lng2), max(lng1, lng2))\n",
    "    return lat, lng\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Create dimension tables: products, sellers, customers, geolocation\n",
    "# -----------------------------\n",
    "\n",
    "N_PRODUCTS = 6000\n",
    "N_SELLERS = 2500\n",
    "N_CUSTOMERS = 120000\n",
    "N_ZIPCODES = 15000\n",
    "\n",
    "# Products\n",
    "product_ids = [rand_id(\"prod\") for _ in range(N_PRODUCTS)]\n",
    "product_rows = []\n",
    "for pid in product_ids:\n",
    "    cat, minp, maxp = random.choice(categories)\n",
    "    name_len = np.random.randint(20, 60)\n",
    "    desc_len = np.random.randint(200, 1200)\n",
    "    photos = np.random.randint(1, 5)\n",
    "    weight = int(np.random.gamma(shape=4, scale=300) + 100)\n",
    "    length = int(np.random.normal(30, 10))\n",
    "    height = int(np.random.normal(15, 6))\n",
    "    width  = int(np.random.normal(25, 8))\n",
    "    product_rows.append([pid, cat, name_len, desc_len, photos, weight, length, height, width])\n",
    "\n",
    "products_df = pd.DataFrame(product_rows, columns=[\n",
    "    \"product_id\",\"product_category_name\",\"product_name_lenght\",\"product_description_lenght\",\n",
    "    \"product_photos_qty\",\"product_weight_g\",\"product_length_cm\",\"product_height_cm\",\"product_width_cm\"\n",
    "])\n",
    "\n",
    "# Sellers\n",
    "seller_ids = [rand_id(\"sell\") for _ in range(N_SELLERS)]\n",
    "seller_rows = []\n",
    "for sid in seller_ids:\n",
    "    st, city = pick_state_city()\n",
    "    seller_rows.append([sid, zip_prefix(), city, st])\n",
    "sellers_df = pd.DataFrame(seller_rows, columns=[\"seller_id\",\"seller_zip_code_prefix\",\"seller_city\",\"seller_state\"])\n",
    "\n",
    "# Customers\n",
    "customer_ids = [rand_id(\"cust\") for _ in range(N_CUSTOMERS)]\n",
    "unique_ids = [rand_id(\"uniq\", 16) for _ in range(N_CUSTOMERS)]\n",
    "customer_rows = []\n",
    "for cid, uid in zip(customer_ids, unique_ids):\n",
    "    st, city = pick_state_city()\n",
    "    customer_rows.append([cid, uid, zip_prefix(), city, st])\n",
    "customers_df = pd.DataFrame(customer_rows, columns=[\n",
    "    \"customer_id\",\"customer_unique_id\",\"customer_zip_code_prefix\",\"customer_city\",\"customer_state\"\n",
    "])\n",
    "\n",
    "# Geolocations (zip prefix to coordinates)\n",
    "zips = random.sample(range(10000, 99999), N_ZIPCODES)\n",
    "geo_rows = []\n",
    "for z in zips:\n",
    "    st, city = pick_state_city()\n",
    "    lat, lng = geo_coord_for_state(st)\n",
    "    geo_rows.append([z, lat, lng, city, st])\n",
    "geolocation_df = pd.DataFrame(geo_rows, columns=[\n",
    "    \"geolocation_zip_code_prefix\",\"geolocation_lat\",\"geolocation_lng\",\"geolocation_city\",\"geolocation_state\"\n",
    "])\n",
    "\n",
    "# Category translations\n",
    "trans_rows = [[c[0], translations[c[0]]] for c in categories]\n",
    "translation_df = pd.DataFrame(trans_rows, columns=[\"product_category_name\",\"product_category_name_english\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Generate orders over time with macro effects\n",
    "# -----------------------------\n",
    "\n",
    "# Daily base orders starting at ~120 and scaled by month index + noise\n",
    "daily_dates = pd.date_range(start_date, end_date, freq=\"D\")\n",
    "daily_orders = []\n",
    "base_daily = 120\n",
    "\n",
    "for d in daily_dates:\n",
    "    month_period = d.to_period(\"M\")\n",
    "    seasonal = 1.0\n",
    "    # weekend boost for online shopping\n",
    "    if d.weekday() in (5, 6):\n",
    "        seasonal *= 1.08\n",
    "    # micro events: prime-like mid-July, hot sales\n",
    "    if d.month == 7 and d.day in range(10,18):\n",
    "        seasonal *= 1.12\n",
    "    # Black Friday week (last Fri of Nov) -> big spike\n",
    "    if d.month == 11 and d.weekday() == 4 and 23 <= d.day <= 29:\n",
    "        seasonal *= 1.6\n",
    "    # Christmas run-up\n",
    "    if d.month == 12 and d.day >= 10:\n",
    "        seasonal *= 1.25\n",
    "    \n",
    "    mu = base_daily * monthly_index.loc[month_period] * seasonal\n",
    "    # Poisson-like variability\n",
    "    count = max(0, int(np.random.normal(mu, mu**0.5)))\n",
    "    daily_orders.append((d, count))\n",
    "\n",
    "orders_per_day = dict(daily_orders)\n",
    "TOTAL_ORDERS = sum(count for _, count in daily_orders)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Build Orders, Items, Payments, Reviews\n",
    "# -----------------------------\n",
    "\n",
    "order_rows = []\n",
    "item_rows  = []\n",
    "payment_rows = []\n",
    "review_rows = []\n",
    "\n",
    "def sample_product():\n",
    "    row = products_df.sample(1).iloc[0]\n",
    "    return row[\"product_id\"], row[\"product_category_name\"]\n",
    "\n",
    "def price_for_category(cat, date):\n",
    "    base = [c for c in categories if c[0]==cat][0]\n",
    "    minp, maxp = base[1], base[2]\n",
    "    year = date.year\n",
    "    infl = inflation_by_year.get(year, 1.0)\n",
    "    p = np.random.uniform(minp, maxp) * infl\n",
    "    return round(float(p), 2)\n",
    "\n",
    "def freight_from_price(price):\n",
    "    # freight roughly 8-15% of price\n",
    "    return round(float(price * np.random.uniform(0.08, 0.15)), 2)\n",
    "\n",
    "def delivery_dates(purchase_dt):\n",
    "    y = purchase_dt.year\n",
    "    base = delay_year_base.get(y, 10)\n",
    "    noise = max(0, int(np.random.normal(0, 3)))\n",
    "    delivered_days = base + noise\n",
    "    est_days = max(2, int(np.random.normal(base-1, 2)))\n",
    "    approved = purchase_dt + pd.to_timedelta(np.random.randint(0, 48), unit=\"h\")\n",
    "    ship = approved + pd.to_timedelta(np.random.randint(12, 72), unit=\"h\")\n",
    "    delivered = ship + pd.to_timedelta(delivered_days, unit=\"D\")\n",
    "    est = purchase_dt + pd.to_timedelta(est_days, unit=\"D\")\n",
    "    return approved, ship, delivered, est\n",
    "\n",
    "def order_status_for_year(y):\n",
    "    r = random.random()\n",
    "    cancel = cancel_rate_year.get(y, 0.03)\n",
    "    if r < cancel:\n",
    "        return \"canceled\"\n",
    "    # a tiny portion \"shipped\" or \"invoiced\"\n",
    "    if r < cancel + 0.03:\n",
    "        return random.choice([\"shipped\",\"invoiced\"])\n",
    "    return \"delivered\"\n",
    "\n",
    "def review_score_for_year(y, delayed=False):\n",
    "    mean = review_mean_by_year.get(y, 4.0)\n",
    "    sd = 0.8\n",
    "    score = int(np.clip(round(np.random.normal(mean - (0.4 if delayed else 0), sd)), 1, 5))\n",
    "    return score\n",
    "\n",
    "# For mapping: pick random customers/sellers weighted by state populations (simple: SP more weight)\n",
    "state_weight = {st: (3.0 if st==\"SP\" else 1.0) for st,_ in states}\n",
    "seller_state = sellers_df[\"seller_state\"].map(state_weight).values\n",
    "seller_probs = seller_state / seller_state.sum()\n",
    "\n",
    "customer_state = customers_df[\"customer_state\"].map(state_weight).values\n",
    "customer_probs = customer_state / customer_state.sum()\n",
    "\n",
    "order_id_seq = 0\n",
    "review_id_seq = 0\n",
    "\n",
    "for d, count in daily_orders:\n",
    "    for _ in range(count):\n",
    "        order_id = rand_id(\"ord\", 20)\n",
    "        # link a customer and pick 1–4 items, possibly multiple sellers\n",
    "        cust_idx = np.random.choice(len(customers_df), p=customer_probs)\n",
    "        cust = customers_df.iloc[cust_idx]\n",
    "        y = d.year\n",
    "        \n",
    "        status = order_status_for_year(y)\n",
    "        approved, ship_dt, delivered_dt, estimated_dt = delivery_dates(pd.Timestamp(d) + pd.to_timedelta(np.random.randint(0, 24), unit=\"h\"))\n",
    "        if status == \"canceled\":\n",
    "            delivered_dt = pd.NaT\n",
    "        \n",
    "        order_rows.append([\n",
    "            order_id, cust[\"customer_id\"], status,\n",
    "            d, approved, ship_dt, delivered_dt, estimated_dt\n",
    "        ])\n",
    "        \n",
    "        # items\n",
    "        n_items = np.random.choice([1,2,3,4], p=[0.65,0.22,0.10,0.03])\n",
    "        for item_no in range(1, n_items+1):\n",
    "            pid, cat = sample_product()\n",
    "            sid = sellers_df.sample(1, weights=seller_probs).iloc[0][\"seller_id\"]\n",
    "            price = price_for_category(cat, d)\n",
    "            freight = freight_from_price(price)\n",
    "            ship_limit = approved + pd.to_timedelta(np.random.randint(6, 48), unit=\"h\")\n",
    "            item_rows.append([order_id, item_no, pid, sid, ship_limit, price, freight])\n",
    "        \n",
    "        # payments (1-2 sequential payments; installments common)\n",
    "        payment_seqs = np.random.choice([1,2], p=[0.85,0.15])\n",
    "        remaining_value = 0.0\n",
    "        # total order value approximated as sum of item prices + freight\n",
    "        # We'll calculate later by grouping to assign payment values realistically\n",
    "        for seq in range(1, payment_seqs+1):\n",
    "            ptype = np.random.choice(\n",
    "                [\"credit_card\",\"boleto\",\"voucher\",\"debit_card\"],\n",
    "                p=[0.70,0.18,0.07,0.05]\n",
    "            )\n",
    "            installments = int(np.random.choice([1,2,3,6,10,12], p=[0.4,0.15,0.15,0.15,0.10,0.05]))\n",
    "            # placeholder payment value; fix later based on order total\n",
    "            payment_rows.append([order_id, seq, ptype, installments, np.nan])\n",
    "        \n",
    "        # reviews only for delivered\n",
    "        if status == \"delivered\" and pd.notnull(delivered_dt):\n",
    "            review_id_seq += 1\n",
    "            delayed = (delivered_dt - estimated_dt) > pd.Timedelta(days=1)\n",
    "            review_score = review_score_for_year(y, delayed=delayed)\n",
    "            # review created near/after delivery\n",
    "            review_created = delivered_dt + pd.to_timedelta(np.random.randint(0, 10), unit=\"D\")\n",
    "            review_answer = review_created + pd.to_timedelta(np.random.randint(0, 5), unit=\"D\")\n",
    "            review_rows.append([\n",
    "                f\"rev_{review_id_seq:08d}\", order_id, review_score, \"\", \"\",\n",
    "                review_created, review_answer\n",
    "            ])\n",
    "\n",
    "# Build DataFrames\n",
    "orders_df = pd.DataFrame(order_rows, columns=[\n",
    "    \"order_id\",\"customer_id\",\"order_status\",\"order_purchase_timestamp\",\"order_approved_at\",\n",
    "    \"order_delivered_carrier_date\",\"order_delivered_customer_date\",\"order_estimated_delivery_date\"\n",
    "])\n",
    "\n",
    "items_df = pd.DataFrame(item_rows, columns=[\n",
    "    \"order_id\",\"order_item_id\",\"product_id\",\"seller_id\",\"shipping_limit_date\",\"price\",\"freight_value\"\n",
    "])\n",
    "\n",
    "payments_df = pd.DataFrame(payment_rows, columns=[\n",
    "    \"order_id\",\"payment_sequential\",\"payment_type\",\"payment_installments\",\"payment_value\"\n",
    "])\n",
    "\n",
    "reviews_df = pd.DataFrame(review_rows, columns=[\n",
    "    \"review_id\",\"order_id\",\"review_score\",\"review_comment_title\",\"review_comment_message\",\n",
    "    \"review_creation_date\",\"review_answer_timestamp\"\n",
    "])\n",
    "\n",
    "# Assign realistic payment_value based on order totals\n",
    "order_totals = items_df.groupby(\"order_id\")[[\"price\",\"freight_value\"]].sum().sum(axis=1)\n",
    "# For missing orders (canceled with no items, unlikely), handle safely\n",
    "order_totals = order_totals.reindex(orders_df[\"order_id\"]).fillna(0.0)\n",
    "\n",
    "# Distribute order total across payment rows per order\n",
    "def allocate_payments(df_pay):\n",
    "    allocated = []\n",
    "    for oid, grp in df_pay.groupby(\"order_id\"):\n",
    "        total = float(order_totals.get(oid, 0.0))\n",
    "        if total <= 0:\n",
    "            vals = [0.0]*len(grp)\n",
    "        else:\n",
    "            shares = np.random.dirichlet(np.ones(len(grp)))\n",
    "            vals = list(np.round(shares * total, 2))\n",
    "            # fix rounding\n",
    "            diff = round(total - sum(vals), 2)\n",
    "            if vals:\n",
    "                vals[0] = round(vals[0] + diff, 2)\n",
    "        g2 = grp.copy()\n",
    "        g2[\"payment_value\"] = vals\n",
    "        allocated.append(g2)\n",
    "    return pd.concat(allocated, ignore_index=True)\n",
    "\n",
    "payments_df = allocate_payments(payments_df)\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Save all CSVs\n",
    "# -----------------------------\n",
    "\n",
    "files = {\n",
    "    \"olist_customers_dataset.csv\": customers_df,\n",
    "    \"olist_orders_dataset.csv\": orders_df,\n",
    "    \"olist_order_items_dataset.csv\": items_df,\n",
    "    \"olist_products_dataset.csv\": products_df,\n",
    "    \"olist_sellers_dataset.csv\": sellers_df,\n",
    "    \"olist_geolocation_dataset.csv\": geolocation_df,\n",
    "    \"olist_order_payments_dataset.csv\": payments_df,\n",
    "    \"olist_order_reviews_dataset.csv\": reviews_df,\n",
    "    \"product_category_name_translation.csv\": translation_df\n",
    "}\n",
    "\n",
    "for fname, df in files.items():\n",
    "    df.to_csv(os.path.join(OUT_DIR, fname), index=False)\n",
    "\n",
    "# Zip it\n",
    "zip_path = \"/mnt/data/olist_synthetic/olist_synthetic_2018_09_to_2023_12.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for fname in files.keys():\n",
    "        zf.write(os.path.join(OUT_DIR, fname), arcname=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9315e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b69f0564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic event-aware Olist dataset generated in: D:\\Portfolio Projects\\Ecommerce\\Data\\olist_synthetic\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "fake = Faker(\"pt_BR\")\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ----------------------------\n",
    "# Helper Functions\n",
    "# ----------------------------\n",
    "def rand_id(prefix, size=10):\n",
    "    return prefix + \"_\" + \"\".join(random.choices(string.ascii_lowercase + string.digits, k=size))\n",
    "\n",
    "def random_date(start, end):\n",
    "    delta = end - start\n",
    "    return start + timedelta(seconds=random.randint(0, int(delta.total_seconds())))\n",
    "\n",
    "# ----------------------------\n",
    "# Simulation parameters\n",
    "# ----------------------------\n",
    "start_date = datetime(2018, 9, 1)\n",
    "end_date   = datetime(2023, 12, 31)\n",
    "\n",
    "n_customers = 50000\n",
    "n_sellers   = 2000\n",
    "n_products  = 5000\n",
    "n_orders    = 120000\n",
    "n_geo       = 3000\n",
    "\n",
    "out_dir = r\"D:\\Portfolio Projects\\Ecommerce\\Data\\olist_synthetic\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Customers\n",
    "# ----------------------------\n",
    "customers = []\n",
    "for i in range(n_customers):\n",
    "    customers.append({\n",
    "        \"customer_id\": rand_id(\"cust\"),\n",
    "        \"customer_unique_id\": rand_id(\"uniq\"),\n",
    "        \"customer_zip_code_prefix\": fake.postcode()[:5],\n",
    "        \"customer_city\": fake.city(),\n",
    "        \"customer_state\": fake.state_abbr()\n",
    "    })\n",
    "customers_df = pd.DataFrame(customers)\n",
    "customers_df.to_csv(os.path.join(out_dir, \"olist_customers_dataset.csv\"), index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Sellers\n",
    "# ----------------------------\n",
    "sellers = []\n",
    "for i in range(n_sellers):\n",
    "    sellers.append({\n",
    "        \"seller_id\": rand_id(\"sell\"),\n",
    "        \"seller_zip_code_prefix\": fake.postcode()[:5],\n",
    "        \"seller_city\": fake.city(),\n",
    "        \"seller_state\": fake.state_abbr()\n",
    "    })\n",
    "sellers_df = pd.DataFrame(sellers)\n",
    "sellers_df.to_csv(os.path.join(out_dir, \"olist_sellers_dataset.csv\"), index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Products\n",
    "# ----------------------------\n",
    "categories = [\n",
    "    \"bed_bath_table\", \"health_beauty\", \"sports_leisure\", \"computers_accessories\",\n",
    "    \"housewares\", \"watches_gifts\", \"telephony\", \"garden_tools\",\n",
    "    \"fashion_bags_accessories\", \"toys\", \"baby\", \"pet_shop\"\n",
    "]\n",
    "products = []\n",
    "for i in range(n_products):\n",
    "    products.append({\n",
    "        \"product_id\": rand_id(\"prod\"),\n",
    "        \"product_category_name\": random.choice(categories),\n",
    "        \"product_name_lenght\": random.randint(20, 60),\n",
    "        \"product_description_lenght\": random.randint(100, 500),\n",
    "        \"product_photos_qty\": random.randint(1, 4),\n",
    "        \"product_weight_g\": random.randint(100, 10000),\n",
    "        \"product_length_cm\": random.randint(10, 100),\n",
    "        \"product_height_cm\": random.randint(5, 60),\n",
    "        \"product_width_cm\": random.randint(5, 60)\n",
    "    })\n",
    "products_df = pd.DataFrame(products)\n",
    "products_df.to_csv(os.path.join(out_dir, \"olist_products_dataset.csv\"), index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Orders + Items + Payments + Reviews\n",
    "# ----------------------------\n",
    "orders, items, payments, reviews = [], [], [], []\n",
    "\n",
    "def adjust_for_economy(order_date, category):\n",
    "    \"\"\"Adjust demand and cancellation probability based on real-world events\"\"\"\n",
    "    year = order_date.year\n",
    "    month = order_date.month\n",
    "\n",
    "    # Base probability of cancellation\n",
    "    cancel_prob = 0.05\n",
    "\n",
    "    # 2020 COVID impact\n",
    "    if year == 2020 and month in [3,4,5]:\n",
    "        if category in [\"fashion_bags_accessories\", \"sports_leisure\", \"watches_gifts\"]:\n",
    "            return 0.5  # massive cancellation\n",
    "        if category in [\"bed_bath_table\", \"computers_accessories\", \"health_beauty\"]:\n",
    "            return 0.02  # strong demand, low cancellations\n",
    "        cancel_prob = 0.15\n",
    "\n",
    "    # Inflation in 2022: slightly higher cancellations\n",
    "    if year == 2022:\n",
    "        cancel_prob += 0.07\n",
    "\n",
    "    return cancel_prob\n",
    "\n",
    "for i in range(n_orders):\n",
    "    order_id = rand_id(\"ord\")\n",
    "    cust = customers_df.sample(1).iloc[0]\n",
    "    order_purchase = random_date(start_date, end_date)\n",
    "\n",
    "    product = products_df.sample(1).iloc[0]\n",
    "    cancel_prob = adjust_for_economy(order_purchase, product.product_category_name)\n",
    "\n",
    "    order_status = \"delivered\" if random.random() > cancel_prob else \"canceled\"\n",
    "\n",
    "    order_approved = order_purchase + timedelta(hours=random.randint(1, 72))\n",
    "    order_delivered = order_approved + timedelta(days=random.randint(1, 15)) if order_status == \"delivered\" else None\n",
    "    order_estimated = order_approved + timedelta(days=random.randint(5, 20))\n",
    "\n",
    "    orders.append({\n",
    "        \"order_id\": order_id,\n",
    "        \"customer_id\": cust.customer_id,\n",
    "        \"order_status\": order_status,\n",
    "        \"order_purchase_timestamp\": order_purchase,\n",
    "        \"order_approved_at\": order_approved,\n",
    "        \"order_delivered_carrier_date\": order_approved + timedelta(days=1),\n",
    "        \"order_delivered_customer_date\": order_delivered,\n",
    "        \"order_estimated_delivery_date\": order_estimated\n",
    "    })\n",
    "\n",
    "    # Items\n",
    "    n_items = random.randint(1, 3)\n",
    "    for j in range(n_items):\n",
    "        seller = sellers_df.sample(1).iloc[0]\n",
    "        price = random.uniform(20, 500)\n",
    "        freight = random.uniform(5, 50)\n",
    "        items.append({\n",
    "            \"order_id\": order_id,\n",
    "            \"order_item_id\": j + 1,\n",
    "            \"product_id\": product.product_id,\n",
    "            \"seller_id\": seller.seller_id,\n",
    "            \"shipping_limit_date\": order_approved + timedelta(days=2),\n",
    "            \"price\": round(price, 2),\n",
    "            \"freight_value\": round(freight, 2)\n",
    "        })\n",
    "\n",
    "    # Payments\n",
    "    payments.append({\n",
    "        \"order_id\": order_id,\n",
    "        \"payment_sequential\": 1,\n",
    "        \"payment_type\": random.choices(\n",
    "            [\"credit_card\", \"boleto\", \"voucher\", \"debit_card\", \"pix\"],\n",
    "            weights=[0.6, 0.15, 0.05, 0.1, 0.1]\n",
    "        )[0],\n",
    "        \"payment_installments\": random.randint(1, 12),\n",
    "        \"payment_value\": round(price + freight, 2)\n",
    "    })\n",
    "\n",
    "    # Reviews\n",
    "    if order_status == \"delivered\" and order_delivered:\n",
    "        reviews.append({\n",
    "            \"review_id\": rand_id(\"rev\"),\n",
    "            \"order_id\": order_id,\n",
    "            \"review_score\": random.choices([1, 2, 3, 4, 5], weights=[0.1, 0.1, 0.2, 0.3, 0.3])[0],\n",
    "            \"review_comment_title\": fake.sentence(nb_words=5),\n",
    "            \"review_comment_message\": fake.text(max_nb_chars=200),\n",
    "            \"review_creation_date\": order_delivered + timedelta(days=random.randint(0, 5)),\n",
    "            \"review_answer_timestamp\": order_delivered + timedelta(days=random.randint(1, 7))\n",
    "        })\n",
    "\n",
    "# Save files\n",
    "pd.DataFrame(orders).to_csv(os.path.join(out_dir, \"olist_orders_dataset.csv\"), index=False)\n",
    "pd.DataFrame(items).to_csv(os.path.join(out_dir, \"olist_order_items_dataset.csv\"), index=False)\n",
    "pd.DataFrame(payments).to_csv(os.path.join(out_dir, \"olist_order_payments_dataset.csv\"), index=False)\n",
    "pd.DataFrame(reviews).to_csv(os.path.join(out_dir, \"olist_order_reviews_dataset.csv\"), index=False)\n",
    "\n",
    "cat_translation = pd.DataFrame({\n",
    "    \"product_category_name\": categories,\n",
    "    \"product_category_name_english\": [c.replace(\"_\", \" \").title() for c in categories]\n",
    "})\n",
    "cat_translation.to_csv(os.path.join(out_dir, \"product_category_name_translation.csv\"), index=False)\n",
    "\n",
    "print(f\"Synthetic event-aware Olist dataset generated in: {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811bf3bb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Downloading faker-37.6.0-py3-none-any.whl (1.9 MB)\n",
      "                                              0.0/1.9 MB ? eta -:--:--\n",
      "                                              0.0/1.9 MB 1.4 MB/s eta 0:00:02\n",
      "                                              0.0/1.9 MB 495.5 kB/s eta 0:00:04\n",
      "     -                                        0.1/1.9 MB 656.4 kB/s eta 0:00:03\n",
      "     --                                       0.1/1.9 MB 656.4 kB/s eta 0:00:03\n",
      "     --                                       0.1/1.9 MB 656.4 kB/s eta 0:00:03\n",
      "     --                                       0.1/1.9 MB 656.4 kB/s eta 0:00:03\n",
      "     --                                       0.1/1.9 MB 426.7 kB/s eta 0:00:05\n",
      "     ---                                      0.2/1.9 MB 499.5 kB/s eta 0:00:04\n",
      "     ---                                      0.2/1.9 MB 472.6 kB/s eta 0:00:04\n",
      "     -----                                    0.3/1.9 MB 610.3 kB/s eta 0:00:03\n",
      "     -------                                  0.4/1.9 MB 696.3 kB/s eta 0:00:03\n",
      "     --------                                 0.4/1.9 MB 748.8 kB/s eta 0:00:03\n",
      "     ---------                                0.5/1.9 MB 761.8 kB/s eta 0:00:02\n",
      "     -----------                              0.6/1.9 MB 880.0 kB/s eta 0:00:02\n",
      "     ------------                             0.6/1.9 MB 864.5 kB/s eta 0:00:02\n",
      "     -------------                            0.6/1.9 MB 868.8 kB/s eta 0:00:02\n",
      "     ---------------                          0.7/1.9 MB 925.1 kB/s eta 0:00:02\n",
      "     ----------------                         0.8/1.9 MB 952.3 kB/s eta 0:00:02\n",
      "     -----------------                        0.8/1.9 MB 936.2 kB/s eta 0:00:02\n",
      "     ------------------                       0.9/1.9 MB 966.2 kB/s eta 0:00:02\n",
      "     -------------------                      1.0/1.9 MB 999.3 kB/s eta 0:00:01\n",
      "     ---------------------                    1.1/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "     ----------------------                   1.1/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "     ------------------------                 1.2/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ------------------------                 1.2/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ------------------------                 1.2/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------                1.3/1.9 MB 999.4 kB/s eta 0:00:01\n",
      "     ---------------------------              1.4/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "     ----------------------------             1.4/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "     -----------------------------            1.5/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     -----------------------------            1.5/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ------------------------------           1.5/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "     --------------------------------         1.6/1.9 MB 1.0 MB/s eta 0:00:01\n",
      "     -----------------------------------      1.7/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     -----------------------------------      1.8/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.9/1.9 MB 926.3 kB/s eta 0:00:00\n",
      "Collecting tzdata (from faker)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: tzdata, faker\n",
      "Successfully installed faker-37.6.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade1137e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
